{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr_Usb4um62a",
    "outputId": "2d359c9b-fd15-4ff2-b8e5-7303de7938d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
      "Collecting dgl\n",
      "  Downloading https://data.dgl.ai/wheels/cu121/dgl-2.0.0%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (926.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.0/926.0 MB\u001b[0m \u001b[31m925.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
      "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata>=0.5.0->dgl) (1.3.0)\n",
      "Installing collected packages: dgl\n",
      "Successfully installed dgl-2.0.0+cu121\n",
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
      "Collecting dglgo\n",
      "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m496.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.9.0)\n",
      "Collecting isort>=5.10.1 (from dglgo)\n",
      "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
      "  Downloading autopep8-2.0.4-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
      "  Downloading numpydoc-1.6.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (2.6.1)\n",
      "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0.1)\n",
      "Collecting ogb>=1.3.3 (from dglgo)\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
      "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
      "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
      "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
      "Requirement already satisfied: sphinx>=5 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (5.0.2)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.3)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.1.0+cu121)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.25.2)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.66.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.7)\n",
      "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.9.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.2.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.5)\n",
      "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2023.4)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.8)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.6)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.0.5)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.1.10)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.7)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.16.1)\n",
      "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.18.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.7.16)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=b7711fc5f18c142449b50f5d17b6c258dca770920fdc9c4cd5e150e20f4e0e14\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, ruamel.yaml, outdated, autopep8, ogb, numpydoc, dglgo\n",
      "Successfully installed autopep8-2.0.4 dglgo-0.0.2 isort-5.13.2 littleutils-0.2.2 numpydoc-1.6.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.11.1 rdkit-pypi-2022.9.5 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install  dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
    "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
    "\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzQSn3iObSox",
    "outputId": "2aa1c08e-e625-42ef-a91d-0e607a632350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mS0bWISTqhMI",
    "outputId": "51836c2d-9c41-4ccd-bdd2-61c781bc4177",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import random\n",
    "import pathlib\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "import dgl.function as fn\n",
    "import dgl.nn as dglnn\n",
    "from dgl import from_networkx\n",
    "from dgl.data.utils import save_graphs\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import category_encoders as ce\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fBP9dONhaRu8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        ### force to outut fix dimensions\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        ### apply weight\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m': self.W_msg(th.cat([edges.src['h'], edges.data['h']], 2))}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Eq4\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Eq5\n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, 128, activation))\n",
    "        self.layers.append(SAGELayer(128, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "        return nfeats.sum(1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(th.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, 2)\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        return self.pred(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SX1H69nKaRu9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Epochs'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "        \n",
    "        \n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def correct_df(df, cols):\n",
    "\n",
    "    df = df[cols]\n",
    "    df['IPV4_SRC_ADDR'] = df.IPV4_SRC_ADDR.apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))\n",
    "    df['IPV4_SRC_ADDR'] = df.IPV4_SRC_ADDR.apply(str)\n",
    "    df['L4_SRC_PORT'] = df.L4_SRC_PORT.apply(str)\n",
    "    df['IPV4_DST_ADDR'] = df.IPV4_DST_ADDR.apply(str)\n",
    "    df['L4_DST_PORT'] = df.L4_DST_PORT.apply(str)\n",
    "    df['IPV4_SRC_ADDR'] = df['IPV4_SRC_ADDR'] + ':' + df['L4_SRC_PORT']\n",
    "    df['IPV4_DST_ADDR'] = df['IPV4_DST_ADDR'] + ':' + df['L4_DST_PORT']\n",
    "    df.drop(columns=['L4_SRC_PORT','L4_DST_PORT'], inplace=True)\n",
    "    df.rename(columns={\"Label\": \"label\"},inplace = True)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    label = df.label\n",
    "    df.drop(columns=['label'],inplace = True)\n",
    "    df = pd.concat([df, label], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_egraphsage(df_train, model, epochs, cuda, report_epochs):\n",
    "\n",
    "    print(\"Building a graph...\")\n",
    "    G = nx.from_pandas_edgelist(df_train, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h','label'],create_using=nx.MultiGraph())\n",
    "    G = G.to_directed()\n",
    "    G = from_networkx(G,edge_attrs=['h','label'] )\n",
    "\n",
    "    G.ndata['h'] = th.ones(G.num_nodes(), G.edata['h'].shape[1])\n",
    "    G.edata['train_mask'] = th.ones(len(G.edata['h']), dtype=th.bool)\n",
    "    G.ndata['h'] = th.reshape(G.ndata['h'], (G.ndata['h'].shape[0], 1,G.ndata['h'].shape[1]))\n",
    "    G.edata['h'] = th.reshape(G.edata['h'], (G.edata['h'].shape[0], 1,G.edata['h'].shape[1]))\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",\n",
    "                                                      classes = np.unique(G.edata['label'].cpu().numpy()),\n",
    "                                                      y = G.edata['label'].cpu().numpy())\n",
    "\n",
    "    class_weights = th.FloatTensor(class_weights).cuda() if cuda else th.FloatTensor(class_weights)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    if cuda:\n",
    "        G = G.to('cuda:0')\n",
    "        model = model.cuda()\n",
    "\n",
    "    node_features = G.ndata['h']\n",
    "    edge_features = G.edata['h']\n",
    "\n",
    "    edge_label = G.edata['label']\n",
    "    train_mask = G.edata['train_mask']\n",
    "\n",
    "    opt = th.optim.Adam(model.parameters())\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    print(\"----------------------\")\n",
    "    #for epoch in range(1,epochs+1):\n",
    "    for epoch in log_progress(range(1,epochs+1), every=1):\n",
    "        pred = model(G, node_features,edge_features).cuda() if cuda else model(G, node_features,edge_features)\n",
    "        loss = criterion(pred[train_mask], edge_label[train_mask])\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % report_epochs == 0:\n",
    "            print('Training acc',str(epoch),':', compute_accuracy(pred[train_mask], edge_label[train_mask]))\n",
    "        \n",
    "    return model, G, node_features, edge_features\n",
    "\n",
    "def train_in_chunks(model, path, chunk_size, cols, epochs, cuda=False, report_epochs=100, limit=None):\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    def extract_number(path):\n",
    "        return int(path.stem.split(\".\")[1])\n",
    "\n",
    "    files = sorted(pathlib.Path(path).glob(\"part.*.parquet\"), key=extract_number)\n",
    "\n",
    "    encoder = None\n",
    "    scaler = None\n",
    "    cols_to_norm = None\n",
    "\n",
    "    df_train = pd.DataFrame()\n",
    "    for index, path in enumerate(files):\n",
    "        \n",
    "        count += df_train.shape[0]\n",
    "        \n",
    "        #print(path)\n",
    "        df_chunk = correct_df(pd.read_parquet(path), cols)\n",
    "        \n",
    "        if index == len(files) - 1 or df_train.shape[0] >= chunk_size:\n",
    "            if encoder == None:\n",
    "                encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "                encoder.fit(df_train, df_train.label)\n",
    "            df_train = encoder.transform(df_train)\n",
    "\n",
    "            if scaler == None:\n",
    "                scaler = StandardScaler()\n",
    "                cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "                df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "            df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "            df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "\n",
    "            model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "\n",
    "            df_train = pd.DataFrame()\n",
    "    \n",
    "        else:\n",
    "            df_train = pd.concat([df_train, df_chunk])\n",
    "            \n",
    "        if limit != None and limit <= count:\n",
    "            break\n",
    "\n",
    "    return model, encoder, scaler, cols_to_norm\n",
    "\n",
    "\n",
    "def create_graph(df_test, encoder, scaler, cols_to_norm, n_features):\n",
    "    df_test = encoder.transform(df_test)\n",
    "    df_test[cols_to_norm] = scaler.transform(df_test[cols_to_norm])\n",
    "    df_test['h'] = df_test[cols_to_norm].values.tolist()\n",
    "\n",
    "    G_test = nx.from_pandas_edgelist(df_test, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", ['h','label'],create_using=nx.MultiGraph())\n",
    "    G_test = G_test.to_directed()\n",
    "    G_test = from_networkx(G_test,edge_attrs=['h','label'] )\n",
    "    actual = G_test.edata.pop('label')\n",
    "    G_test.ndata['feature'] = th.ones(G_test.num_nodes(), n_features)\n",
    "\n",
    "    G_test.ndata['feature'] = th.reshape(G_test.ndata['feature'], (G_test.ndata['feature'].shape[0], 1, G_test.ndata['feature'].shape[1]))\n",
    "\n",
    "    G_test.edata['h'] = th.reshape(G_test.edata['h'], (G_test.edata['h'].shape[0], 1, G_test.edata['h'].shape[1]))\n",
    "\n",
    "    return G_test, actual\n",
    "    \n",
    "\n",
    "def predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, G_test=None, actual=None):\n",
    "\n",
    "    if G_test == None:\n",
    "        G_test, actual = create_graph(df_test, encoder, scaler, cols_to_norm, n_features)\n",
    "    \n",
    "    node_features_test = G_test.ndata['feature']\n",
    "    edge_features_test = G_test.edata['h']\n",
    "\n",
    "    test_pred = model(G_test, node_features_test, edge_features_test)#.cuda()\n",
    "    test_pred = test_pred.argmax(1)\n",
    "    test_pred = th.Tensor.cpu(test_pred).detach().numpy()\n",
    "\n",
    "    return confusion_matrix(actual, test_pred), accuracy_score(actual, test_pred), G_test, actual\n",
    "\n",
    "def save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH):\n",
    "    th.save(model, MODEL_PATH)\n",
    "    if scaler != None:\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "    if encoder != None:\n",
    "        joblib.dump(encoder, ENCODER_PATH)\n",
    "\n",
    "def save_graph(G, GRAPH_PATH):\n",
    "    save_graphs(GRAPH_PATH, [G], None)\n",
    "\n",
    "def load_model(MODEL_PATH, SCALER_PATH, ENCODER_PATH):\n",
    "    model = th.load(MODEL_PATH)\n",
    "    model.eval()\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    encoder = joblib.load(ENCODER_PATH)\n",
    "\n",
    "    return model, scaler, encoder\n",
    "\n",
    "def load_graph(G, GRAPH_PATH):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XUIP8u7kaRu9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1E__R0BNZTs"
   },
   "source": [
    "**COLAB directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1L_B5XNNBU9"
   },
   "outputs": [],
   "source": [
    "dir_data = '/content/drive/MyDrive/csci_e-599a/data/'\n",
    "dir_model = '/content/drive/MyDrive/csci_e-599a/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yno-iYAfNdO-"
   },
   "source": [
    "**Local directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HnMqGLOoNGkv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_data = '../data/netflow/parquet/original/'\n",
    "dir_model = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== NF-BoT-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be5b61a63d9472e9c6850c343696498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9248425364494324\n",
      "Saving model: model/nf-bot-iot_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8123    981]\n",
      " [   313 580805]]\n",
      "0.9978076045962366\n",
      "\n",
      "Incremental data training...\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1e9a810bf24be791f8bbbbb07b67dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9312680959701538\n",
      "Saving model: model/0_nf-bot-iot_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8125    979]\n",
      " [   204 580914]]\n",
      "0.9979956694260803\n",
      "\n",
      "==== NF-ToN-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a9d8573514f639e42fd16a9d133f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9956914782524109\n",
      "Saving model: model/nf-ton-iot_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8915    189]\n",
      " [411384 169734]]\n",
      "0.30268102510580763\n",
      "\n",
      "Incremental data training...\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee063dd33aa948f7832e37e4ee3cc2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9805534482002258\n",
      "Saving model: model/1_nf-ton-iot_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8919    185]\n",
      " [482012  99106]]\n",
      "0.18302435354832589\n",
      "\n",
      "==== NF-UNSW-NB15_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27573593e2f48b0a89c476077026d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9777935147285461\n",
      "Saving model: model/nf-unsw-nb15_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8912    192]\n",
      " [402033 179085]]\n",
      "0.3185191334785894\n",
      "\n",
      "Incremental data training...\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7978ea1cc4454629912b4b488869a9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9762794971466064\n",
      "Saving model: model/2_nf-unsw-nb15_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  8989    115]\n",
      " [416169 164949]]\n",
      "0.2946992826428022\n",
      "\n",
      "==== NF-UQ-NIDS_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522409882ba948889c851c97f6f2fba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9429799914360046\n",
      "Saving model: model/nf-uq-nids_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  6356   2748]\n",
      " [353736 227382]]\n",
      "0.39601709187390505\n",
      "\n",
      "Incremental data training...\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b83565a1e6417d890947f511dcbff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9648990035057068\n",
      "Saving model: model/3_nf-uq-nids_chunks.pt\n",
      "\n",
      "Test results:\n",
      "[[  4848   4256]\n",
      " [342937 238181]]\n",
      "0.4117586264151455\n",
      "\n",
      "==== NF-CSE-CIC-IDS2018_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182e7ca4f294e26bca5b3d730d5dcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(n_features, \u001b[38;5;241m128\u001b[39m, n_features, F\u001b[38;5;241m.\u001b[39mrelu, dropouts)\n\u001b[0;32m---> 64\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_egraphsage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m MODEL_PATH)\n\u001b[1;32m     66\u001b[0m save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
      "Cell \u001b[0;32mIn[3], line 117\u001b[0m, in \u001b[0;36mtrain_egraphsage\u001b[0;34m(df_train, model, epochs, cuda, report_epochs)\u001b[0m\n\u001b[1;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred[train_mask], edge_label[train_mask])\n\u001b[1;32m    116\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 117\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m report_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py_jop/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_jop/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_jop/lib/python3.10/site-packages/torch/autograd/function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netflows = ['NF-BoT-IoT_chunks',\n",
    "           'NF-ToN-IoT_chunks',\n",
    "           'NF-UNSW-NB15_chunks',\n",
    "           'NF-UQ-NIDS_chunks',\n",
    "           'NF-CSE-CIC-IDS2018_chunks',\n",
    "           'NF-BoT-IoT-v2_chunks',\n",
    "           'NF-ToN-IoT-v2_chunks',\n",
    "           'NF-UNSW-NB15-v2_chunks',\n",
    "           'NF-UQ-NIDS-v2_chunks',\n",
    "           'NF-CSE-CIC-IDS2018-v2_chunks']\n",
    "\n",
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']\n",
    "\n",
    "n_features = len(cols) - 4 - 1 #'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'Label'\n",
    "encoder = None\n",
    "scaler = None\n",
    "cols_to_norm = None\n",
    "\n",
    "\n",
    "chunk_size = 1000000\n",
    "epochs = 1000\n",
    "cuda = False\n",
    "report_epochs = epochs\n",
    "\n",
    "df_test = correct_df(pd.read_parquet(dir_data + 'Attack-2_chunks'), cols)\n",
    "G_test = None\n",
    "actual = None\n",
    "\n",
    "dropouts = 0.2\n",
    "model_ = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "\n",
    "\n",
    "for i, nf in enumerate(netflows):\n",
    "    print (\"\\n=====================================================\")\n",
    "    print (\"\\n==== \" + nf + \" ====\")\n",
    "    \n",
    "    MODEL_PATH = dir_model + nf.lower() + '.pt'\n",
    "    SCALER_PATH = dir_model + '_scaler_' + nf.lower() + '.pkl'\n",
    "    ENCODER_PATH = dir_model + '_encoder_' + nf.lower() + '.pkl'\n",
    "    PATH = dir_data + nf\n",
    "    \n",
    "    MODEL_PATH_ = dir_model + str(i)+\"_\"+nf.lower() + '.pt'\n",
    "    \n",
    "    df_train = pd.read_parquet(PATH)\n",
    "    sample_size = min(chunk_size, df_train.shape[0])\n",
    "    df_train = df_train.sample(n=sample_size, random_state=1)\n",
    "    df_train = correct_df(df_train, cols)\n",
    "    \n",
    "    if encoder == None:\n",
    "        encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "        encoder.fit(df_train, df_train.label)\n",
    "    df_train = encoder.transform(df_train)\n",
    "\n",
    "    if scaler == None:\n",
    "        scaler = StandardScaler()\n",
    "        cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "        df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "    df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "    df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "    \n",
    "    model = None\n",
    "    model = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "    model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "    print(\"Saving model: \" + MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
    "    \n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, G_test, actual)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)\n",
    "    \n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"Incremental data training...\")\n",
    "    model_, G_, node_features_, edge_features_ = train_egraphsage(df_train, model_, epochs, cuda, report_epochs)\n",
    "    save_model(model_, MODEL_PATH_, None, None, None, None)\n",
    "    print(\"Saving model: \" + MODEL_PATH_)\n",
    "    \n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model_, encoder, scaler, cols_to_norm, n_features, G_test, actual)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoT Graph from 100,000 edges, only 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "==== NF-BoT-IoT_chunks ====\n",
      "Building a graph...\n"
     ]
    }
   ],
   "source": [
    "netflows = ['NF-BoT-IoT_chunks']\n",
    "\n",
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']\n",
    "\n",
    "n_features = len(cols) - 4 - 1 #'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'Label'\n",
    "\n",
    "chunk_size = 100000\n",
    "epochs = 200\n",
    "cuda = False\n",
    "report_epochs = epochs\n",
    "\n",
    "df_test = correct_df(pd.read_parquet(dir_data + 'Attack-2_chunks'), cols)\n",
    "\n",
    "dropouts = 0.2\n",
    "\n",
    "for i, nf in enumerate(netflows):\n",
    "    print (\"\\n==============================\")\n",
    "    print (\"==== \" + nf + \" ====\")\n",
    "    \n",
    "    MODEL_PATH = dir_model + nf.lower() + '.pt'\n",
    "    SCALER_PATH = dir_model + '_scaler_' + nf.lower() + '.pkl'\n",
    "    ENCODER_PATH = dir_model + '_encoder_' + nf.lower() + '.pkl'\n",
    "    PATH = dir_data + nf\n",
    "    \n",
    "    MODEL_PATH_ = dir_model + str(i)+\"_\"+nf.lower() + '.pt'\n",
    "    \n",
    "    df_train = pd.read_parquet(PATH)\n",
    "    sample_size = min(chunk_size, df_train.shape[0])\n",
    "    df_train = df_train.sample(n=sample_size, random_state=1)\n",
    "    df_train = correct_df(df_train, cols)\n",
    "    \n",
    "    encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "    encoder.fit(df_train, df_train.label)\n",
    "    df_train = encoder.transform(df_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "    df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "    df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "    df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "    \n",
    "    model = None\n",
    "    model = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "    model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "    print(\"Saving model: \" + MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
    "    \n",
    "    print(\"Predicting on 'Attack-2'\")\n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoT Graph from 1,000,000 edges, 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "==== NF-BoT-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b39e657ba040a7af339072feac6c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 1000 : 0.9297717213630676\n",
      "Saving model: model/nf-bot-iot_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  8123    981]\n",
      " [   178 580940]]\n",
      "0.9980363320919925\n"
     ]
    }
   ],
   "source": [
    "netflows = ['NF-BoT-IoT_chunks']\n",
    "\n",
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']\n",
    "\n",
    "n_features = len(cols) - 4 - 1 #'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'Label'\n",
    "\n",
    "chunk_size = 1000000\n",
    "epochs = 1000\n",
    "cuda = False\n",
    "report_epochs = epochs\n",
    "\n",
    "df_test = correct_df(pd.read_parquet(dir_data + 'Attack-2_chunks'), cols)\n",
    "\n",
    "dropouts = 0.2\n",
    "\n",
    "for i, nf in enumerate(netflows):\n",
    "    print (\"\\n==============================\")\n",
    "    print (\"==== \" + nf + \" ====\")\n",
    "    \n",
    "    MODEL_PATH = dir_model + nf.lower() + '.pt'\n",
    "    SCALER_PATH = dir_model + '_scaler_' + nf.lower() + '.pkl'\n",
    "    ENCODER_PATH = dir_model + '_encoder_' + nf.lower() + '.pkl'\n",
    "    PATH = dir_data + nf\n",
    "    \n",
    "    MODEL_PATH_ = dir_model + str(i)+\"_\"+nf.lower() + '.pt'\n",
    "    \n",
    "    df_train = pd.read_parquet(PATH)\n",
    "    sample_size = min(chunk_size, df_train.shape[0])\n",
    "    df_train = df_train.sample(n=sample_size, random_state=1)\n",
    "    df_train = correct_df(df_train, cols)\n",
    "    \n",
    "    encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "    encoder.fit(df_train, df_train.label)\n",
    "    df_train = encoder.transform(df_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "    df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "    df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "    df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "    \n",
    "    model = None\n",
    "    model = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "    model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "    print(\"Saving model: \" + MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
    "    \n",
    "    print(\"Predicting on 'Attack-2'\")\n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoT Graph from All edges, 5000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "==== NF-BoT-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5220841dd4410fa0a0c18464530744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=5000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 5000 : 0.9371230006217957\n",
      "Saving model: model/nf-bot-iot_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  8126    978]\n",
      " [   332 580786]]\n",
      "0.9977804961522952\n"
     ]
    }
   ],
   "source": [
    "netflows = ['NF-BoT-IoT_chunks']\n",
    "\n",
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']\n",
    "\n",
    "n_features = len(cols) - 4 - 1 #'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'Label'\n",
    "\n",
    "chunk_size = 100000000\n",
    "epochs = 5000\n",
    "cuda = False\n",
    "report_epochs = epochs\n",
    "\n",
    "df_test = correct_df(pd.read_parquet(dir_data + 'Attack-2_chunks'), cols)\n",
    "\n",
    "dropouts = 0.2\n",
    "\n",
    "for i, nf in enumerate(netflows):\n",
    "    print (\"\\n==============================\")\n",
    "    print (\"==== \" + nf + \" ====\")\n",
    "    \n",
    "    MODEL_PATH = dir_model + nf.lower() + '.pt'\n",
    "    SCALER_PATH = dir_model + '_scaler_' + nf.lower() + '.pkl'\n",
    "    ENCODER_PATH = dir_model + '_encoder_' + nf.lower() + '.pkl'\n",
    "    PATH = dir_data + nf\n",
    "    \n",
    "    MODEL_PATH_ = dir_model + str(i)+\"_\"+nf.lower() + '.pt'\n",
    "    \n",
    "    df_train = pd.read_parquet(PATH)\n",
    "    sample_size = min(chunk_size, df_train.shape[0])\n",
    "    df_train = df_train.sample(n=sample_size, random_state=1)\n",
    "    df_train = correct_df(df_train, cols)\n",
    "    \n",
    "    encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "    encoder.fit(df_train, df_train.label)\n",
    "    df_train = encoder.transform(df_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "    df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "    df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "    df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "    \n",
    "    model = None\n",
    "    model = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "    model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "    print(\"Saving model: \" + MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
    "    \n",
    "    print(\"Predicting on 'Attack-2'\")\n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs from 100,000 edges all datasets, only 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "==== NF-BoT-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55d2fae0c5549d9a07ab1780351a4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.8572149872779846\n",
      "Saving model: model/nf-bot-iot_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  5494   3610]\n",
      " [   155 580963]]\n",
      "0.9936210442850317\n",
      "\n",
      "==============================\n",
      "==== NF-ToN-IoT_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7b572cfd9c4242896d8fd4b08b52e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9157350063323975\n",
      "Saving model: model/nf-ton-iot_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  8811    293]\n",
      " [ 99805 481313]]\n",
      "0.8304061861469074\n",
      "\n",
      "==============================\n",
      "==== NF-UNSW-NB15_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5880151b3f247ebafee0143a3cf9c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9785199761390686\n",
      "Saving model: model/nf-unsw-nb15_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[     0   9104]\n",
      " [    46 581072]]\n",
      "0.9844973586209934\n",
      "\n",
      "==============================\n",
      "==== NF-UQ-NIDS_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f162216bcf52469dba4919d4fdfe60d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9257599711418152\n",
      "Saving model: model/nf-uq-nids_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  8649    455]\n",
      " [500743  80375]]\n",
      "0.1508313820901288\n",
      "\n",
      "==============================\n",
      "==== NF-CSE-CIC-IDS2018_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1db410ec674a4ba857c18f74269969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.8702700138092041\n",
      "Saving model: model/nf-cse-cic-ids2018_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  9104      0]\n",
      " [528313  52805]]\n",
      "0.1048910409981329\n",
      "\n",
      "==============================\n",
      "==== NF-BoT-IoT-v2_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89742a55b62464bac0995b78ebdf831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9592049717903137\n",
      "Saving model: model/nf-bot-iot-v2_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  9019     85]\n",
      " [318137 262981]]\n",
      "0.46084354700434754\n",
      "\n",
      "==============================\n",
      "==== NF-ToN-IoT-v2_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5753c3a2b57f4dd4a9adb0e042b5e130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.7136449813842773\n",
      "Saving model: model/nf-ton-iot-v2_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  4065   5039]\n",
      " [   409 580709]]\n",
      "0.9907695748379424\n",
      "\n",
      "==============================\n",
      "==== NF-UNSW-NB15-v2_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8961afd2c8e74c758b59fa9add52a628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9874250292778015\n",
      "Saving model: model/nf-unsw-nb15-v2_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[   569   8535]\n",
      " [577601   3517]]\n",
      "0.00692281887154325\n",
      "\n",
      "==============================\n",
      "==== NF-UQ-NIDS-v2_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6eb09e6e584a3fb0a3215aa8f3c1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.8045750260353088\n",
      "Saving model: model/nf-uq-nids-v2_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[    13   9091]\n",
      " [    16 581102]]\n",
      "0.9845702125640861\n",
      "\n",
      "==============================\n",
      "==== NF-CSE-CIC-IDS2018-v2_chunks ====\n",
      "Building a graph...\n",
      "Training started...\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76be11bd14434c1f8e5c2f1244a8743c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc 200 : 0.9544699788093567\n",
      "Saving model: model/nf-cse-cic-ids2018-v2_chunks.pt\n",
      "Predicting on 'Attack-2'\n",
      "\n",
      "Test results:\n",
      "[[  8952    152]\n",
      " [567954  13164]]\n",
      "0.03747064663804467\n"
     ]
    }
   ],
   "source": [
    "netflows = ['NF-BoT-IoT_chunks',\n",
    "           'NF-ToN-IoT_chunks',\n",
    "           'NF-UNSW-NB15_chunks',\n",
    "           'NF-UQ-NIDS_chunks',\n",
    "           'NF-CSE-CIC-IDS2018_chunks',\n",
    "           'NF-BoT-IoT-v2_chunks',\n",
    "           'NF-ToN-IoT-v2_chunks',\n",
    "           'NF-UNSW-NB15-v2_chunks',\n",
    "           'NF-UQ-NIDS-v2_chunks',\n",
    "           'NF-CSE-CIC-IDS2018-v2_chunks']\n",
    "\n",
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']\n",
    "\n",
    "n_features = len(cols) - 4 - 1 #'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'Label'\n",
    "\n",
    "chunk_size = 100000\n",
    "epochs = 200\n",
    "cuda = False\n",
    "report_epochs = epochs\n",
    "\n",
    "df_test = correct_df(pd.read_parquet(dir_data + 'Attack-2_chunks'), cols)\n",
    "\n",
    "dropouts = 0.2\n",
    "\n",
    "for i, nf in enumerate(netflows):\n",
    "    print (\"\\n==============================\")\n",
    "    print (\"==== \" + nf + \" ====\")\n",
    "    \n",
    "    MODEL_PATH = dir_model + nf.lower() + '.pt'\n",
    "    SCALER_PATH = dir_model + '_scaler_' + nf.lower() + '.pkl'\n",
    "    ENCODER_PATH = dir_model + '_encoder_' + nf.lower() + '.pkl'\n",
    "    PATH = dir_data + nf\n",
    "    \n",
    "    MODEL_PATH_ = dir_model + str(i)+\"_\"+nf.lower() + '.pt'\n",
    "    \n",
    "    df_train = pd.read_parquet(PATH)\n",
    "    sample_size = min(chunk_size, df_train.shape[0])\n",
    "    df_train = df_train.sample(n=sample_size, random_state=1)\n",
    "    df_train = correct_df(df_train, cols)\n",
    "    \n",
    "    encoder = ce.TargetEncoder(cols=['TCP_FLAGS','PROTOCOL'])\n",
    "    encoder.fit(df_train, df_train.label)\n",
    "    df_train = encoder.transform(df_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_norm = list(set(list(df_train.iloc[:, 2:].columns ))  - set(list(['label'])) )\n",
    "    df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
    "    df_train[cols_to_norm] = scaler.transform(df_train[cols_to_norm])\n",
    "\n",
    "    df_train['h'] = df_train[cols_to_norm].values.tolist()\n",
    "    \n",
    "    model = None\n",
    "    model = Model(n_features, 128, n_features, F.relu, dropouts)\n",
    "    model, G, node_features, edge_features = train_egraphsage(df_train, model, epochs, cuda, report_epochs)\n",
    "    print(\"Saving model: \" + MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH, scaler, SCALER_PATH, encoder, ENCODER_PATH)\n",
    "    \n",
    "    print(\"Predicting on 'Attack-2'\")\n",
    "    cm, acc, G_test, actual = predict_egraphsage(df_test, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "    print(\"\\nTest results:\")\n",
    "    print(cm)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "df_attack = pd.read_parquet(dir_data + 'attack2/attack')\n",
    "df_bgrd = pd.read_parquet(dir_data + 'attack2/background')\n",
    "\n",
    "df_attack.label = 1\n",
    "df_bgrd.label = 0\n",
    "\n",
    "#Imbalanced\n",
    "df_imb = pd.concat([df_attack, df_bgrd]).reset_index(drop=True)\n",
    "df_imb = df_imb.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_imb['FLOW_DURATION_MILLISECONDS'] = ((pd.to_datetime(df_imb['t_last']) - df_imb['t_first'] + timedelta(hours=4, minutes=0)).dt.total_seconds() * 10**3).astype(int)\n",
    "\n",
    "def flag_to_numeric(flag):\n",
    "    return int(''.join(['0' if s == '.' else '1' for s in flag]), 2)\n",
    "\n",
    "df_imb['TCP_FLAGS'] = df_imb.tcp_flags.apply(flag_to_numeric)\n",
    "\n",
    "df_imb.drop(columns=['t_first','t_last','src6_addr','src_tos','dst6_addr','icmp_code',\n",
    "                   'icmp_status','sampled','export_sysid','fwd_status','app_latency',\n",
    "                   'cli_latency','srv_latency', 'tcp_flags'], inplace = True)\n",
    "df_imb.columns = ['IPV4_SRC_ADDR', 'L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT',\n",
    "       'IN_BYTES', 'IN_PKTS', 'Label', 'PROTOCOL', 'OUT_BYTES', 'OUT_PKTS',\n",
    "       'FLOW_DURATION_MILLISECONDS', 'TCP_FLAGS']\n",
    "df_imb = df_imb.drop_duplicates()\n",
    "\n",
    "\n",
    "df_b_b = df_imb[df_imb['Label']==0]\n",
    "df_b_a = df_imb[df_imb['Label']==1]\n",
    "\n",
    "#balanced\n",
    "df_b = pd.concat([df_b_a.sample(n=df_b_b.shape[0], random_state=1), df_b_b]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_imb = correct_df(df_imb, cols)\n",
    "df_b = correct_df(df_b, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results (Imbalanced):\n",
      "[[  8120    984]\n",
      " [   176 580942]]\n",
      "0.9980346378142462\n"
     ]
    }
   ],
   "source": [
    "cm, acc, G_test, actual = predict_egraphsage(df_imb, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "print(\"\\nTest results (Imbalanced):\")\n",
    "print(cm)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results (Balanced):\n",
      "[[8115  989]\n",
      " [   0 9104]]\n",
      "0.9456832161687171\n"
     ]
    }
   ],
   "source": [
    "cm, acc, G_test, actual = predict_egraphsage(df_b, model, encoder, scaler, cols_to_norm, n_features, None, None)\n",
    "\n",
    "print(\"\\nTest results (Balanced):\")\n",
    "print(cm)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = model.gnn(G, node_features, edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.8667, 0.0000,  ..., 0.0514, 6.7053, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.4395, 4.5555, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 3.3647, 3.1353, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.1056, 0.7814, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.7058, 3.9165, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.1566, 2.8912, 0.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bot_emb = pd.DataFrame(embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.866689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.199146</td>\n",
       "      <td>6.120585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.134766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.543159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>6.312424</td>\n",
       "      <td>3.163555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.806769</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>6.705304</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.444332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.505930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.460386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.439473</td>\n",
       "      <td>4.555478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.858747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.965234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088603</td>\n",
       "      <td>4.867826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.364663</td>\n",
       "      <td>3.135283</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.712359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.147270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.880036</td>\n",
       "      <td>0.316823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.137607</td>\n",
       "      <td>5.074939</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.632731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.214451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230374</td>\n",
       "      <td>4.064116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.950132</td>\n",
       "      <td>3.276883</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.071394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.290052</td>\n",
       "      <td>13.043841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.371132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.652094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909483</td>\n",
       "      <td>29.564411</td>\n",
       "      <td>8.192632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.259212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.955686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150230</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.780759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501232</td>\n",
       "      <td>0.132170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.445663</td>\n",
       "      <td>1.369995</td>\n",
       "      <td>0.863999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107186</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.025115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.711297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710734</td>\n",
       "      <td>0.360713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483158</td>\n",
       "      <td>1.105566</td>\n",
       "      <td>0.781391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.560455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.576202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.270420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.705842</td>\n",
       "      <td>3.916513</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.492635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.769264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.570386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.156592</td>\n",
       "      <td>2.891197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107189 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1    2          3          4    5    6          7    8    \\\n",
       "0       0.0  1.866689  0.0   4.199146   6.120585  0.0  0.0   8.134766  0.0   \n",
       "1       0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   5.444332  0.0   \n",
       "2       0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   4.858747  0.0   \n",
       "3       0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   5.712359  0.0   \n",
       "4       0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   4.632731  0.0   \n",
       "...     ...       ...  ...        ...        ...  ...  ...        ...  ...   \n",
       "107184  0.0  9.071394  0.0  18.290052  13.043841  0.0  0.0  22.371132  0.0   \n",
       "107185  0.0  0.000000  0.0   0.150230   0.010867  0.0  0.0   0.898572  0.0   \n",
       "107186  0.0  0.000000  0.0   0.310355   0.000000  0.0  0.0   1.025115  0.0   \n",
       "107187  0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   5.560455  0.0   \n",
       "107188  0.0  0.000000  0.0   0.000000   0.000000  0.0  0.0   4.492635  0.0   \n",
       "\n",
       "              9    ...       118       119        120       121  122  123  \\\n",
       "0        8.543159  ...  0.000000  0.838766   6.312424  3.163555  0.0  0.0   \n",
       "1        7.505930  ...  0.000000  4.460386   0.000000  0.000000  0.0  0.0   \n",
       "2        7.965234  ...  0.088603  4.867826   0.000000  0.000000  0.0  0.0   \n",
       "3        8.147270  ...  0.000000  4.880036   0.316823  0.000000  0.0  0.0   \n",
       "4        8.214451  ...  0.230374  4.064116   0.000000  0.000000  0.0  0.0   \n",
       "...           ...  ...       ...       ...        ...       ...  ...  ...   \n",
       "107184  30.652094  ...  0.000000  0.909483  29.564411  8.192632  0.0  0.0   \n",
       "107185   1.780759  ...  0.000000  0.501232   0.132170  0.000000  0.0  0.0   \n",
       "107186   1.711297  ...  0.000000  0.710734   0.360713  0.000000  0.0  0.0   \n",
       "107187   8.576202  ...  0.000000  4.270420   0.000000  0.000000  0.0  0.0   \n",
       "107188   7.769264  ...  0.000000  3.570386   0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "             124       125        126  127  \n",
       "0       5.806769  0.051384   6.705304  0.0  \n",
       "1       0.000000  2.439473   4.555478  0.0  \n",
       "2       0.000000  3.364663   3.135283  0.0  \n",
       "3       0.000000  3.137607   5.074939  0.0  \n",
       "4       0.000000  3.950132   3.276883  0.0  \n",
       "...          ...       ...        ...  ...  \n",
       "107184  6.259212  0.000000  15.955686  0.0  \n",
       "107185  0.445663  1.369995   0.863999  0.0  \n",
       "107186  0.483158  1.105566   0.781391  0.0  \n",
       "107187  0.000000  2.705842   3.916513  0.0  \n",
       "107188  0.000000  2.156592   2.891197  0.0  \n",
       "\n",
       "[107189 rows x 128 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600100, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_parquet(dir_data+'NF-BoT-IoT_chunks')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
