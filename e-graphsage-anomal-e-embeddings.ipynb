{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hjc3iIihKLn-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mammadli/miniconda3/envs/py_jop/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 13.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.data import DGLDataset\n",
    "import time\n",
    "import networkx as nx\n",
    "import category_encoders as ce\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from typing import *\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_apply = nn.Linear(ndim_in + edims , ndim_out)\n",
    "        self.activation = F.relu\n",
    "        self.W_edge = nn.Linear(128 * 2, 256)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.W_apply.weight, gain=gain)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'m':  edges.data['h']}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            g.ndata['h'] = F.relu(self.W_apply(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "\n",
    "            # Compute edge embeddings\n",
    "            u, v = g.edges()\n",
    "            edge = self.W_edge(torch.cat((g.srcdata['h'][u], g.dstdata['h'][v]), 2))\n",
    "            return g.ndata['h'], edge\n",
    "        \n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim,  activation):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, 128, F.relu))\n",
    "\n",
    "    def forward(self, g, nfeats, efeats, corrupt=False):\n",
    "        if corrupt:\n",
    "            e_perm = torch.randperm(g.number_of_edges())\n",
    "            #n_perm = torch.randperm(g.number_of_nodes())\n",
    "            efeats = efeats[e_perm]\n",
    "            #nfeats = nfeats[n_perm]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            #nfeats = layer(g, nfeats, efeats)\n",
    "            nfeats, e_feats = layer(g, nfeats, efeats)\n",
    "        #return nfeats.sum(1)\n",
    "        return nfeats.sum(1), e_feats.sum(1)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def uniform(self, size, tensor):\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        if tensor is not None:\n",
    "            tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.weight.size(0)\n",
    "        self.uniform(size, self.weight)\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        features = torch.matmul(features, torch.matmul(self.weight, summary))\n",
    "        return features\n",
    "    \n",
    "class DGI(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation):\n",
    "        super(DGI, self).__init__()\n",
    "        self.encoder = SAGE(ndim_in, ndim_out, edim,  F.relu)\n",
    "        #self.discriminator = Discriminator(128)\n",
    "        self.discriminator = Discriminator(256)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, g, n_features, e_features):\n",
    "        positive = self.encoder(g, n_features, e_features, corrupt=False)\n",
    "        negative = self.encoder(g, n_features, e_features, corrupt=True)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, g, n_features, e_features):\n",
    "        positive = self.encoder(g, n_features, e_features, corrupt=False)\n",
    "        negative = self.encoder(g, n_features, e_features, corrupt=True)\n",
    "\n",
    "        positive = positive[1]\n",
    "        negative = negative[1]\n",
    "\n",
    "        summary = torch.sigmoid(positive.mean(dim=0))\n",
    "\n",
    "        positive = self.discriminator(positive, summary)\n",
    "        negative = self.discriminator(negative, summary)\n",
    "\n",
    "        l1 = self.loss(positive, torch.ones_like(positive))\n",
    "        l2 = self.loss(negative, torch.zeros_like(negative))\n",
    "\n",
    "        return l1 + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_df(df, cols, with_port=True):\n",
    "    \n",
    "    df = df[cols]\n",
    "    #df['IPV4_SRC_ADDR'] = df.IPV4_SRC_ADDR.apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))\n",
    "    df['IPV4_SRC_ADDR'] = df.IPV4_SRC_ADDR.apply(str)\n",
    "    df['IPV4_DST_ADDR'] = df.IPV4_DST_ADDR.apply(str)\n",
    "    if with_port:\n",
    "        df['L4_SRC_PORT'] = df.L4_SRC_PORT.apply(str)\n",
    "        df['L4_DST_PORT'] = df.L4_DST_PORT.apply(str)\n",
    "        df['IPV4_SRC_ADDR'] = df['IPV4_SRC_ADDR'] + ':' + df['L4_SRC_PORT']\n",
    "        df['IPV4_DST_ADDR'] = df['IPV4_DST_ADDR'] + ':' + df['L4_DST_PORT']\n",
    "    df.drop(columns=['L4_SRC_PORT','L4_DST_PORT'], inplace=True)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    sample_size = min(df.shape[0], 2000000)\n",
    "    print(\"Number of EDGEs: \" + str(sample_size))\n",
    "    \n",
    "    return df.sample(n=sample_size, random_state=1)\n",
    "\n",
    "#enc_cols=['TCP_FLAGS','PROTOCOL']\n",
    "def create_graph(data, enc_cols):\n",
    "\n",
    "    X_train = data.drop(columns=[\"Label\"])\n",
    "    y_train = data[[\"Label\"]]\n",
    "\n",
    "    encoder = ce.TargetEncoder(cols=enc_cols)#enc_cols\n",
    "    encoder.fit(X_train, y_train.Label)\n",
    "\n",
    "    X_train = encoder.transform(X_train)\n",
    "\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_train.fillna(0, inplace=True)\n",
    "\n",
    "    scaler = Normalizer()\n",
    "    cols_to_norm = list(set(list(X_train.iloc[:, 2:].columns))) # Ignore first two as the represents IP addresses\n",
    "    scaler.fit(X_train[cols_to_norm])\n",
    "\n",
    "    # Transform on training set\n",
    "    X_train[cols_to_norm] = scaler.transform(X_train[cols_to_norm])\n",
    "    X_train['h'] = X_train.iloc[:, 2:].values.tolist()\n",
    "\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Training graph\n",
    "    train_g = nx.from_pandas_edgelist(train, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\",\n",
    "                [\"h\", \"Label\"], create_using=nx.MultiGraph())\n",
    "\n",
    "    train_g = train_g.to_directed()\n",
    "    train_g = dgl.from_networkx(train_g, edge_attrs=['h', 'Label'])\n",
    "    nfeat_weight = torch.ones([train_g.number_of_nodes(),\n",
    "                            train_g.edata['h'].shape[1]])\n",
    "    train_g.ndata['h'] = nfeat_weight\n",
    "\n",
    "    return train_g #, test_g\n",
    "\n",
    "def train_dgi(train_g, epochs, file_name, dir_model):\n",
    "    \n",
    "    ndim_in = train_g.ndata['h'].shape[1]\n",
    "    ndim_out = 128\n",
    "    edim = train_g.edata['h'].shape[1]\n",
    "\n",
    "    dgi = DGI(ndim_in, ndim_out, edim, F.relu)\n",
    "\n",
    "    dgi_optimizer = torch.optim.Adam(dgi.parameters(), lr=1e-3, weight_decay=0.)\n",
    "\n",
    "    train_g.ndata['h'] = torch.reshape(train_g.ndata['h'],\n",
    "                                       (train_g.ndata['h'].shape[0], 1,\n",
    "                                        train_g.ndata['h'].shape[1]))\n",
    "    train_g.edata['h'] = torch.reshape(train_g.edata['h'],\n",
    "                                       (train_g.edata['h'].shape[0], 1,\n",
    "                                        train_g.edata['h'].shape[1]))\n",
    "\n",
    "    best = 1e9\n",
    "    node_features = train_g.ndata['h']\n",
    "    edge_features = train_g.edata['h']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        dgi.train()\n",
    "\n",
    "        dgi_optimizer.zero_grad()\n",
    "        loss = dgi(train_g, node_features, edge_features)\n",
    "        loss.backward()\n",
    "        dgi_optimizer.step()\n",
    "\n",
    "        if loss < best:\n",
    "            best = loss\n",
    "            torch.save(dgi.state_dict(), dir_model+'/dgi_'+file_name+'.pkl')\n",
    "\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(\"Epoch {:05d} | Loss {:.4f}\".format(epoch, loss.item()))\n",
    "    \n",
    "    print(\"Best Loss {:.4f}\".format(loss.item()))\n",
    "    \n",
    "    return dgi\n",
    "            \n",
    "def load_dgi(file_name, dir_model):\n",
    "    #dgi.load_state_dict(torch.load('dgi/'+file_name+'.pkl'))\n",
    "    return torch.load(dir_model+'/dgi_'+file_name+'.pkl')\n",
    "\n",
    "\n",
    "def df_embeddings(dgi, G):\n",
    "    emb = dgi.encoder(G, G.ndata['h'], G.edata['h'])[1]\n",
    "    emb = emb.detach().cpu().numpy()\n",
    "\n",
    "    df_emb = pd.DataFrame(emb, )\n",
    "    df_emb[\"Label\"] = G.edata['Label'].detach().cpu().numpy()\n",
    "    \n",
    "    return df_emb\n",
    "\n",
    "def save_embeddings(df_emb, path):\n",
    "    \n",
    "    col_names = [\"emb_\"+str(col) for col in range(256)]\n",
    "    col_names.append(\"Label\")\n",
    "    df_emb.columns = col_names\n",
    "    \n",
    "    size99MB = int(df_emb.memory_usage().sum()/1e6/99) + 1\n",
    "    dd_df = dd.from_pandas(df_emb, npartitions=size99MB)\n",
    "    dd_df.to_parquet(path, compression=\"gzip\")\n",
    "    print(\"Saved to: '\"+path)\n",
    "\n",
    "def create_embeddings(dir_data, file_name, cols, with_port, epochs, emb_prefix, dir_model):\n",
    "    \n",
    "    print(\"Loading data... '\"+dir_data+file_name+\"'\")\n",
    "    df = pd.read_parquet(dir_data+file_name)\n",
    "    \n",
    "    print(\"Correcting DataFrame...\")\n",
    "    df = correct_df(df, cols, with_port)\n",
    "    \n",
    "    print(\"Creating Graphs...\")\n",
    "    enc_cols=['TCP_FLAGS','PROTOCOL']\n",
    "    G = create_graph(df, enc_cols)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    dgi = train_dgi(G, epochs, file_name, dir_model)\n",
    "    dgi.load_state_dict(torch.load(dir_model+'/dgi_'+file_name+'.pkl'))\n",
    "    #dgi = load_dgi(file_name)\n",
    "    \n",
    "    print(\"Generating Embeddings...\")\n",
    "    df_emb = df_embeddings(dgi, G)\n",
    "    #df_embeddings(dgi, G)\n",
    "    \n",
    "    print(\"Saving Embeddings...\")\n",
    "    save_embeddings(df_emb, dir_data + emb_prefix + file_name)\n",
    "    \n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#dir_data = '/content/drive/MyDrive/csci_e-599a/data/'\n",
    "#dir_model = '/content/drive/MyDrive/csci_e-599a/model/'\n",
    "\n",
    "dir_data = '../data/netflow/parquet/original/'\n",
    "dir_model = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netflow_data = ['NF-BoT-IoT_chunks',\n",
    "           'NF-ToN-IoT_chunks',\n",
    "           'NF-UNSW-NB15_chunks',\n",
    "           'NF-UQ-NIDS_chunks',\n",
    "           'NF-CSE-CIC-IDS2018_chunks',\n",
    "           'NF-BoT-IoT-v2_chunks',\n",
    "           'NF-ToN-IoT-v2_chunks',\n",
    "           'NF-UNSW-NB15-v2_chunks',\n",
    "           'NF-UQ-NIDS-v2_chunks',\n",
    "           'NF-CSE-CIC-IDS2018-v2_chunks', \n",
    "           'Attack-2_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netflow_data = ['Attack-2_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT','L4_DST_PORT', 'PROTOCOL', 'IN_BYTES', 'OUT_BYTES',\n",
    "       'IN_PKTS', 'OUT_PKTS', 'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS','Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... '../data/netflow/parquet/original/Attack-2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 295111\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Best Loss 0.6214\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_port_Attack-2_chunks\n"
     ]
    }
   ],
   "source": [
    "with_port = True\n",
    "emb_prefix = \"emb_port_\"\n",
    "for i, data_folder in enumerate(netflow_data):\n",
    "    \n",
    "    df_emb = create_embeddings(dir_data, data_folder, cols, with_port, epochs, emb_prefix, dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HYBqJ8y14mlh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... '../data/netflow/parquet/original/NF-UQ-NIDS_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 2000000\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.3834\n",
      "Epoch 00050 | Loss 0.1932\n",
      "Epoch 00100 | Loss 0.0876\n",
      "Epoch 00150 | Loss 0.0850\n",
      "Epoch 00200 | Loss 0.0796\n",
      "Best Loss 0.0796\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-UQ-NIDS_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-CSE-CIC-IDS2018_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 2000000\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.4570\n",
      "Epoch 00050 | Loss 1.3054\n",
      "Epoch 00100 | Loss 0.5051\n",
      "Epoch 00150 | Loss 0.2293\n",
      "Epoch 00200 | Loss 0.2075\n",
      "Best Loss 0.2075\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-CSE-CIC-IDS2018_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-BoT-IoT-v2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 92173\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.3919\n",
      "Epoch 00050 | Loss 1.3822\n",
      "Epoch 00100 | Loss 1.3430\n",
      "Epoch 00150 | Loss 1.2726\n",
      "Epoch 00200 | Loss 1.0742\n",
      "Best Loss 1.0742\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-BoT-IoT-v2_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-ToN-IoT-v2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 961535\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.4760\n",
      "Epoch 00050 | Loss 1.0890\n",
      "Epoch 00100 | Loss 0.4384\n",
      "Epoch 00150 | Loss 0.3013\n",
      "Epoch 00200 | Loss 0.1326\n",
      "Best Loss 0.1326\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-ToN-IoT-v2_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-UNSW-NB15-v2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 353934\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.3972\n",
      "Epoch 00050 | Loss 1.2709\n",
      "Epoch 00100 | Loss 0.4020\n",
      "Epoch 00150 | Loss 0.0348\n",
      "Epoch 00200 | Loss 0.0147\n",
      "Best Loss 0.0147\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-UNSW-NB15-v2_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-UQ-NIDS-v2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 2000000\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.5780\n",
      "Epoch 00050 | Loss 0.6950\n",
      "Epoch 00100 | Loss 0.3790\n",
      "Epoch 00150 | Loss 0.2954\n",
      "Epoch 00200 | Loss 0.2278\n",
      "Best Loss 0.2278\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-UQ-NIDS-v2_chunks\n",
      "Loading data... '../data/netflow/parquet/original/NF-CSE-CIC-IDS2018-v2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 2000000\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.3911\n",
      "Epoch 00050 | Loss 1.1026\n",
      "Epoch 00100 | Loss 0.4854\n",
      "Epoch 00150 | Loss 0.4246\n",
      "Epoch 00200 | Loss 0.3815\n",
      "Best Loss 0.3815\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_NF-CSE-CIC-IDS2018-v2_chunks\n",
      "Loading data... '../data/netflow/parquet/original/Attack-2_chunks'\n",
      "Correcting DataFrame...\n",
      "Number of EDGEs: 18008\n",
      "Creating Graphs...\n",
      "Training...\n",
      "Epoch 00000 | Loss 1.6324\n",
      "Epoch 00050 | Loss 1.0186\n",
      "Epoch 00100 | Loss 0.1561\n",
      "Epoch 00150 | Loss 0.0989\n",
      "Epoch 00200 | Loss 0.0795\n",
      "Best Loss 0.0795\n",
      "Generating Embeddings...\n",
      "Saving Embeddings...\n",
      "Saved to: '../data/netflow/parquet/original/emb_wo_port_Attack-2_chunks\n"
     ]
    }
   ],
   "source": [
    "with_port = False\n",
    "emb_prefix = \"emb_wo_port_\"\n",
    "\n",
    "for i, data_folder in enumerate(netflow_data):\n",
    "    \n",
    "    df_emb = create_embeddings(dir_data, data_folder, cols, with_port, epochs, emb_prefix, dir_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
